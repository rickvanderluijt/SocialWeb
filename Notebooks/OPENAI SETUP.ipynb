{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b934e47-21e2-4a32-a339-9522acf87595",
   "metadata": {},
   "source": [
    "**OPENAI API PROMPT SETUP**\n",
    "\n",
    "This notebook makes use of the Chatgpt API and prompts this LLM to asks for the political stance in Mastodon Toots. Notebook can be altered to use different dataset or different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1a678-334f-4932-b728-82092c4fe05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas langchain openai\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4da0398-4fa1-4574-bc08-227c9b7f85f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from openai==0.28) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from openai==0.28) (3.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.20->openai==0.28) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->openai==0.28) (1.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rickv\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/76.5 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 71.7/76.5 kB 787.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.5/76.5 kB 608.3 kB/s eta 0:00:00\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.1\n",
      "    Uninstalling openai-1.57.1:\n",
      "      Successfully uninstalled openai-1.57.1\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.2.10 requires openai<2.0.0,>=1.54.0, but you have openai 0.28.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da4bafd-7c62-4af4-bd51-a5162995efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rickv\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de5c89b-8bbb-4066-bb67-63da8fad6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the API with your own key, Key will not be published since confidential information is related to this key. \n",
    "OPENAI_KEY= 'placeholder'\n",
    "OPENAI_API_VERSION='2024-02-15-preview'\n",
    "MODEL=\"gpt-3.5-turbo-16k\"\n",
    "MODEL_TEMPERATURE=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2796cc63-abef-4921-b5f2-f5baee1e8008",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOPENAI_KEY\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:551\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    550\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "# LLM is initalized with variables stated above.\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=0,\n",
    "    api_key = OPENAI_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4ea8c0-a247-458e-81d7-dad915c2e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "file_path = \"All_toots_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c1f34e2-b316-4c01-a0ae-b02b16712f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit processing to the first 10 entries TEST !!\n",
    "# df_limited = df.head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0748a952-1ed8-4dd5-af57-d32ec488a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message to set the task for the model\n",
    "system_message = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a political analyst. Based on the given message content, classify the political standpoint strictly as one of the following: \"\n",
    "        \"'left', 'right', 'center' or 'N/A' if the classification is unclear. Respond with only one of these words.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "319eafbc-183d-4898-81f0-536ed99ad0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify political standpoint\n",
    "def classify_political_standpoint(Content):\n",
    "    messages = [\n",
    "        system_message,\n",
    "        HumanMessage(content=f\"Message: {Content}\\n\\nWhat is the political standpoint?\")\n",
    "    ]\n",
    "    response = llm(messages)\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0c7476-6c35-4602-8480-cc3a3320bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the messages and classify their political standpoint\n",
    "df['political_standpoint'] = df['Content'].apply(classify_political_standpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd46ab82-a206-44e9-accf-7068d57b8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['political_standpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657803e-504f-493b-a032-51aa1a5f57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ''\n",
    "df_limited.to_csv(os.path.join(folder_path, \"All_toots_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395df1f-f701-4a3e-b524-d42216cb07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ''\n",
    "df.to_csv(os.path.join(folder_path, \"All_toots_processed1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cd8d3-6fae-4ed7-836d-2979da84c1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd80e9bd-8b68-4c07-8db3-72587d93f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_72c66423f1b50dcc52b5eacc8dad2a0a in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_72c66423f1b50dcc52b5eacc8dad2a0a in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_72c66423f1b50dcc52b5eacc8dad2a0a in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 10 Dec 2024 17:49:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '369', 'Connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-processing-ms': '71', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '6192', 'x-ratelimit-remaining-tokens': '199752', 'x-ratelimit-reset-requests': '9h8m17.436s', 'x-ratelimit-reset-tokens': '74ms', 'x-request-id': 'req_72c66423f1b50dcc52b5eacc8dad2a0a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'CF-Cache-Status': 'DYNAMIC', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8eff11da5ea4d595-AMS', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "An error occurred: This model's maximum context length is 16385 tokens. However, your messages resulted in 18566 tokens. Please reduce the length of the messages.\n",
      "An error occurred: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_71f3ffafde62dd987e44be902ccb4be9 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_71f3ffafde62dd987e44be902ccb4be9 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_71f3ffafde62dd987e44be902ccb4be9 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 10 Dec 2024 18:11:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '369', 'Connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-processing-ms': '96', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '4420', 'x-ratelimit-remaining-tokens': '199795', 'x-ratelimit-reset-requests': '13h23m28.411s', 'x-ratelimit-reset-tokens': '61ms', 'x-request-id': 'req_71f3ffafde62dd987e44be902ccb4be9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'CF-Cache-Status': 'DYNAMIC', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8eff321abaef9ff6-AMS', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "An error occurred: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_ffaf83dc9dd97671a0932f9c596f642b in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_ffaf83dc9dd97671a0932f9c596f642b in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_ffaf83dc9dd97671a0932f9c596f642b in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 10 Dec 2024 18:18:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '369', 'Connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-processing-ms': '238', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '3770', 'x-ratelimit-remaining-tokens': '199827', 'x-ratelimit-reset-requests': '14h57m1.98s', 'x-ratelimit-reset-tokens': '51ms', 'x-request-id': 'req_ffaf83dc9dd97671a0932f9c596f642b', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'CF-Cache-Status': 'DYNAMIC', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8eff3d8dee989f6a-AMS', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "       Username                                            Content  \\\n",
      "0    helladeboo  Acuut stoppen van #medicatie omdat dit niet me...   \n",
      "1  RonjaBiernat  Audio-documentaire: \"Geen kleine man\"\"70 tot 9...   \n",
      "2    ErikJonker  Er zitten veel aannames en verwachtingen in on...   \n",
      "3    ErikJonker  Het kabinet was zo blij met de overeenstemming...   \n",
      "4    ErikJonker  De invloed van Maurice de Hond is gezien de ve...   \n",
      "\n",
      "             Date/Time DateTime political_standpoint  \n",
      "0  2024-11-30 22:11:52      NaN                  N/A  \n",
      "1  2024-10-08 19:04:10      NaN                  N/A  \n",
      "2  2024-09-17 12:38:31      NaN                  N/A  \n",
      "3  2024-09-15 07:00:46      NaN                 left  \n",
      "4  2024-09-13 13:07:20      NaN                 Left  \n"
     ]
    }
   ],
   "source": [
    "# Code to run trough the entire dataset that is provided in batches to reduce API token costs.\n",
    "openai.api_key = 'placeholder'\n",
    "\n",
    "# Function to classify political standpoint for a single content\n",
    "def classify_political_standpoint(content):\n",
    "    system_message = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a political analyst. Based on the given message content, classify the political standpoint strictly as one of the following: 'left', 'right', 'center' or 'N/A' if the classification is unclear. Respond with only one of these words. Please try to steer clear as much as possible of the answer N/A.\"\n",
    "    }\n",
    "\n",
    "    user_message = {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"Message: {content}\\n\\nWhat is the political standpoint?\"\n",
    "    }\n",
    "\n",
    "    messages = [system_message, user_message]\n",
    "\n",
    "    try:\n",
    "        # Call the OpenAI API for classification\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",  # Adjust model version as needed\n",
    "            messages=messages\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()  # Extract the classification result\n",
    "    except openai.error.RateLimitError:\n",
    "        print(\"Rate limit hit. Sleeping for 10 seconds...\")\n",
    "        time.sleep(10)  # Sleep and retry if rate-limited\n",
    "        return classify_political_standpoint(content)  # Retry after waiting\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"N/A\"  # Return N/A if there is an error\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = \"all_toots_cleaned.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Iterate through each content entry one by one and classify\n",
    "political_standpoints = []\n",
    "for content in df['Content']:\n",
    "    standpoint = classify_political_standpoint(content)\n",
    "    political_standpoints.append(standpoint)\n",
    "\n",
    "    \n",
    "# Add results to DataFrame\n",
    "df['political_standpoint'] = political_standpoints\n",
    "\n",
    "# Save the DataFrame back to CSV\n",
    "df.to_csv(\"all_toots_with_standpoint_V2.csv\", index=False)\n",
    "\n",
    "# Show the first few rows with the new column\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bef56e34-ac99-461e-8a41-23ecba6cba18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32355f-3260-4270-8baa-abd9b8d861d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
